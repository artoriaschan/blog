---
title: 算法与数据结构
subtitle: 整体结构与复杂度分析
date: 2020-04-05 00:00:00
permalink: /pages/d533f9/
sidebar: auto
categories: 
  - 基础知识
tags: 
  - Algorithm
  - Data Structure
---
## 整体结构
![data-structure-and-algorithm-thinking](~@assets/posts/algorithm-abstract/data-structure-and-algorithm-thinking.png)
## 复杂度
数据结构和算法本身解决的是 `快` 和 `省` 的问题，即如何让代码运行得 `更快` ，如何让代码 `更省` 存储空间。

所以， `执行效率` 是算法一个非常重要的 `考量指标` 。那如何来衡量你编写的算法代码的 `执行效率` 呢？

这里就要用到我们今天要讲的内容： `时间` 、 `空间` 复杂度分析。
## 大O复杂度表示法

所有代码的执行时间 $T(n)$ 与每行代码的执行次数 $n$ 成正比。

我们可以把这个规律总结成一个公式：

$$T(n) = O(f(n))$$

其中，$T(n)$ 我们已经讲过了，它表示代码执行的时间；$n$ 表示数据规模的大小；$f(n)$ 表示每行代码执行的次数总和。

因为这是一个公式，所以用 $f(n)$ 来表示。公式中的 $O$，表示代码的执行时间 $T(n)$ 与 $f(n)$ 表达式成正比。

大 $O$ 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的 `变化趋势` ，所以，也叫作 `渐进时间复杂度` ( `asymptotic time complexity` ) ，简称 `时间复杂度` 。
## 时间复杂度分析
### 只关注循环执行次数最多的一段代码
大 $O$ 这种复杂度表示方法只是表示一种变化趋势。

我们通常会忽略掉公式中的 `常量` 、 `低阶` 、 `系数` ，只需要记录一个最大阶的量级就可以了。

所以，我们在分析一个算法、一段代码的时间复杂度的时候，也 **只关注循环执行次数最多的那一段代码** 就可以了。

比如：
```javascript
function calc(n) {
  let sum = 0;
  for(let i = 1; i <= n; i ++>) {
    sum += i;
  }
  return sum;
}
```
其中第 2 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。

循环执行次数最多的是第 3、4 行代码，所以这块代码要重点分析。

前面我们也讲过，这两行代码被执行了 $n$ 次，所以总的时间复杂度就是 $O(n)$。
### 加法法则：总复杂度等于量级最大的那段代码的复杂度
比如如下的代码：
```javascript
function calc(n) {
  let sum1 = 0;

  for(let i = 1; i <= 100; i ++>) {
    sum1 += i;
  }
  
  let sum2 = 0;
  for(let i = 1; i <= n; i ++>) {
    sum2 += i;
  }
  
  let sum3 = 0;
  for(let i = 1; i <= n; i ++>) {
    for(let j = 1; j <= n; j ++>) {
      sum3 += i * j;
    }
  }
  
  return sum1 + sum2+ sum3;
}
```
这个代码分为三部分，分别是求 `sum1` 、 `sum2` 、 `sum3` 。我们可以分别分析每一部分的 `时间复杂度` ，然后把它们放到一块儿，再取一个 `量级最大` 的作为整段代码的复杂度。

第一段代码循环执行了 100 次，所以是一个常量的执行时间，跟 $n$ 的规模无关。时间复杂度为 $O(1)$ 。

那第二段代码和第三段代码的 `时间复杂度` 是多少呢？答案是 $O(n)$ 和 $O(n^2)$ 。

综合这三段代码的时间复杂度，我们取其中 `最大的量级` 。所以，整段代码的时间复杂度就为 $O(n^2)$ 。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度** 。
### 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
看如下代码：
```javascript
function f(n){
  let sum = 0;
  for(let i = 1; i <= n; i ++>) {
    sum += i;
  }
  return n;
}
function calc(){
  let res = 0;
  for(let i = 1; i <= n; i ++>) {
    res += f(i);
  }
  return res;
}
```
我们单独看 `cal` 函数。假设 `f` 只是一个普通的操作，那第 `10～12` 行的时间复杂度就是， $T_1(n) = O(n)$。

但 `f` 函数本身不是一个简单的操作，它的时间复杂度是 $T_2(n) = O(n)$ 。 

所以，整个 `cal` 函数的时间复杂度就是，$T(n) = T_1(n) * T_2(n) = O(n*n) = O(n^2)$。
## 几种常见时间复杂度实例分析
对于复杂度量级，我们可以粗略地分为两类， `多项式量级` 和 `非多项式量级` 。其中， `非多项式量级` 只有两个：$O(2^n)$ 和 $O(n!)$ 。

当数据规模 `n` 越来越大时， `非多项式量级` 算法的执行时间会 `急剧增加` ，求解问题的执行时间会无限增长。所以，**非多项式时间复杂度的算法其实是非常低效的算法**。
### $O(1)$
只要代码的执行时间不随` n `的增大而增长，这样代码的时间复杂度我们都记作 $O(1)$。

或者说，**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是 $Ο(1)$**。
### $O(log_{2}n)$ / $O(nlog_{2}n)$
`对数阶时间复杂度` 非常常见，同时也是 `最难分析` 的一种时间复杂度。

我们下面举例说明：
```javascript
let i = 1;
while(i <= n) {
  i = i * 2
}
```
根据我们前面讲的复杂度分析方法，第 3 行代码是循环执行次数最多的。所以，我们只要能计算出这行代码被执行了多少次，就能知道整段代码的 `时间复杂度` 。

有等比数列的只是我们可以得出 最终执行多少次是由 $2^x = n$ 决定的。

通过 $2^x = n$ 求解 `x` 这个问题我们想高中应该就学过了： $x = log_{2}n$。

所以，这段代码的 `时间复杂度` 就是 $O(log_2 n)$。

实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 $O(logn)$ 。为什么呢？

我们知道，对数之间是可以互相转换的，$log_3 n$ 就等于 $log_3 2 * log_2 n$，所以 $O(log_3 n) = O(C * log_2 n)$，其中 $C=log_3 2$ 是一个常量。

基于我们前面的一个理论：在采用大 $O$ 标记复杂度的时候，可以 `忽略系数` ，即 $O(Cf(n)) = O(f(n))$。

如果你理解了我前面讲的 $O(logn)$ ，那 $O(nlogn)$ 就很容易理解了。还记得我们刚讲的 `乘法法则` 吗？如果一段代码的 `时间复杂度` 是 $O(logn)$，我们循环执行 `n` 遍， `时间复杂度` 就是 $O(nlogn)$ 了。

而且，$O(nlogn)$ 也是一种非常 `常见` 的算法 `时间复杂度` 。比如， `归并排序` 、 `快速排序` 的 `时间复杂度` 都是 $O(nlogn)$ 。
### $O(m + n)$ / $O(m * n)$
我们再来讲一种跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定。

我们先看代码：
```javascript
function calc(m, n){
  let sum1 = 0;
  for(let i = 1; i <= m; i ++>) {
    sum1 += i;
  }
  
  let sum2 = 0;
  for(let i = 1; i <= n; i ++>) {
    sum2 += i;
  }
  
  return sum1 + sum2;
}
```
从代码中可以看出， `m` 和 `n` 是表示两个数据规模。

我们无法事先评估 `m` 和 `n` 谁的量级 大，所以我们在表示复杂度的时候，就不能简单地利用 `加法法则` ，省略掉其中一个。

所以，上面代码的时间复杂度就是 $O(m+n)$。
## 空间复杂度分析
`时间复杂度` 的全称是 `渐进时间复杂度` ，表示算法的执行 `时间` 与数据 `规模` 之间的 `增长关系` 。

类比一下， `空间复杂度` 全称就是 `渐进空间复杂度` ( `asymptotic space complexity` )，表示算法的存储 `空间` 与数据 `规模` 之间的 `增长关系` 。
## 不同情况下的时间复杂度
根据不同情况，我们讲四个复杂度分析方面的知识点：
* `最好情况` 时间复杂度（best case time complexity）
* `最坏情况` 时间复杂度（worst case time complexity）
* `平均情况` 时间复杂度（average case time complexity）
* `均摊` 时间复杂度（amortized time complexity）
### 最好、最坏情况时间复杂度
首先我们先看一下代码示例：
```javascript
function find(arr, item){
  const len = arr.length;
  for(let i = 0; i < len; i ++) {
    if(arr[i] === item) return i;
  }
  return -1;
}
```
这段代码的时间复杂度还是 $O(n)$ 吗？很显然，咱们上一节讲的分析方法，解决不了这个问题。

因为，要查找的变量 x 可能出现在数组的任意位置：
* 如果数组中第一个元素正好是要查找的变量 `x`，那就不需要继续遍历剩下的 `n-1` 个数据了，那 `时间复杂度` 就是 $O(1)$ 。
* 但如果数组中不存在变量 `x`，那我们就需要把整个数组都遍历一遍， `时间复杂度` 就成了 $O(n)$ 。

所以，不同的情况下，这段代码的 `时间复杂度` 是不一样的。

为了表示代码在 `不同情况` 下的 `不同时间复杂度` ，我们需要引入三个概念： `最好情况时间复杂度` 、 `最坏情况时间复杂度` 和 `平均情况时间复杂度` 。

顾名思义，**最好情况时间复杂度就是，在最理想的情况下，执行这段代码的时间复杂度**。

同理， **最坏情况时间复杂度就是，在最糟糕的情况下，执行这段代码的时间复杂度**。
### 平均情况时间复杂度
借助刚才查找变量 item 的例子来给解释 平均情况时间复杂度 。

我们知道，要查找的变量 item，要么在数组里，要么就不在数组里。

这两种情况对应的概率统计起来很麻烦，为了方便你理解，我们假设在数组中与不在数组中的概率都为 $\frac{1}{2}$。

另外，要查找的数据出现在 `0 ～ n - 1` 这 `n` 个位置的概率也是一样的，为 $\frac{1}{n}$ 。

所以，根据 概率乘法法则 ，要查找的数据出现在 `0 ～ n - 1` 中任意位置的概率就是 $\frac{1}{2n}$。

如果 我们把每种情况发生的概率也考虑进去，那平均时间复杂度的计算过程就变成了这样：

$$ 1 * \frac{1}{2n} + 2 * \frac{1}{2n} + 3 * \frac{1}{2n} +\cdots+ n * \frac{1}{2n} + n * \frac{1}{2} = \frac{3n + 1}{4}$$

这个值就是概率论中的 `加权平均值` ，也叫作 `期望值` ，所以 `平均时间复杂度` 的全称应该叫 `加权平均时间复杂度` 或者 `期望时间复杂度` 。

引入概率之后，前面那段代码的 `加权平均值` 为 $\frac{3n + 1}{4}$ 。用大 $O$ 表示法来表示，去掉 `系数` 和 `常量` ，这段代码的 `加权平均时间复杂度` 仍然是 $O(n)$。
### 均摊时间复杂度
到此为止，你应该已经掌握了算法复杂度分析的大部分内容了。下面我要给你讲一个更加高级的概念， `均摊时间复杂度` ，以及它对应的 `分析方法` ： `摊还分析` （或者叫平摊分析）。

我们借助一个例子：
```javascript
const array = [];
const limit = n;
let count = 0;

function insert(val){
  if(count === limit) {
    let sum = 0;
    for(let i = 0; i < array.length; i ++) {
      sum += array[i];
    }
    array [0] = sum;
    count = 1;
  }
  array[count] = val;
  count ++;
}
```
这段代码实现了一个往数组中插入数据的功能。

当数组满了之后，也就是代码中的 `count === limit` 时，我们用 `for` 循环遍历数组求和，并清空数组，将求和之后的 `sum` 值放到数组的第一个位置，然后再将新的数据插入。

但如果数组一开始就有空闲空间，则直接将数据插入数组。

最理想的情况下，数组中有空闲空间，我们只需要将数据插入到数组下标为 `count` 的位置就可以了，所以 `最好情况时间复杂度` 为 $O(1)$ 。

最坏的情况下，数组中没有空闲空间了，我们需要先做一次数组的遍历求和，然后再将数据插入，所以 `最坏情况时间复杂度` 为 $O(n)$ 。 

那 `平均时间复杂度` 是多少呢？答案是 $O(1)$ 。

我们还是可以通过前面讲的概率论的方法来分析。假设数组的长度是 `n` ，根据数据插入的位置的不同，我们可以分为 `n` 种情况，每种情况的 时间复杂度是 $O(1)$。

除此之外，还有一种 `额外` 的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 $O(n)$ 。而且，这 `n + 1` 种情况发生的概率一样，都是 $\frac{1}{n+1}$。

所以，根据 `加权平均` 的计算方法，我们求得的 `平均时间复杂度` 就是：

$$ 1 * \frac{1}{n+1} + 1 * \frac{1}{n+1} + 1 * \frac{1}{n+1} +\cdots+ 1 * \frac{1}{n+1} + n * \frac{1}{n+1} = O(1)$$

就例子中的场景我们可以看出，与传统的 平均情况时间复杂度 还是有很大区别的。 我们对比 insert 函数和上文中的 find 函数来看：
* 首先， `find` 函数在极端情况下，复杂度才为 $O(1)$ 。但 `insert` 函数在大部分情况下，时间复杂度都为 $O(1)$ 。只有个别情况下，复杂度才比较高，为 $O(n)$ 。这是 `insert` 第一个区别于 `find` 的地方。
* 对于 `insert` 函数来说，$O(1)$ 时间复杂度的插入和 $O(n)$ 时间复杂度的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 $O(n)$ 插入之后，紧跟着 `n - 1` 个 $O(1)$ 的插入操作，循环往复。

针对这种特殊的场景，我们引入了一种更加简单的分析方法： `摊还分析法` ，通过 `摊还分析` 得到的时间复杂度我们起了一个名字，叫 `均摊时间复杂度` 。

对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上。而且，**在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度**。

**均摊时间复杂度就是一种特殊的平均时间复杂度**，我们没必要花太多精力去区分它们。应该掌握的是它的分析方法， `摊还分析` 。至于分析出来的结果是叫平均还是叫均摊，这只是个说法，并不重要。